{"meta":{"title":"Emjay's DailyCommit Blog","subtitle":"Emjay's DataScience & Development","description":"매일 공부한 것, 알게된 것을 기록하는 블로그입니다. 또한 제가 알고 있는 지식을 찾아보기 위해 개인 위키처럼 사용하고 있습니다.","author":"EmjayAhn(Minjae Ahn)","url":"https://emjayahn.github.io"},"pages":[],"posts":[{"title":"Requests","slug":"Requests","date":"2018-10-24T03:02:50.000Z","updated":"2018-10-24T14:53:58.922Z","comments":true,"path":"2018/10/24/Requests/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/Requests/","excerpt":"","text":"WEB CRAWLING 1Requests Requests 패키지는 크롤링 하고자 하는 페이지를 url 값을 통해 가져와 객체로 변환 해주는 기능을 가지고 있다. 1. Installation Requests 를 이용하기 위해 package 를 설치해준다.1$ pip3 install requests request를 이용하면서, json 형식으로의 크롤링과, html 형식으로 css selecting 을 위한 크롤링을 실습해 볼 것이므로, BeautifulSoup Package 역시 같이 설치해준다. (BeautifulSoup은 html parser 를 이용해보기 위함이다.) python-forecastio 는 dark sky 의 api를 활용해 날씨 데이터를 받아올때 사용해보기 위해 설치한다. 12$ pip3 install bs4$ pip3 install python-forecastio import 할 것들 1234import requestsimport forecastiofrom bs4 import BeautifulSoupfrom pandas.io.json import json_normalize 2. [ jSON ] Dark Sky api 활용 날씨정보 가져오기 DarkSky api 는 위도와 경도를 입력하면, 날씨 정보를 주는 api 이다. https://darsky.net/dev/ 에 가입 후, TOKEN 을 받는다. 위에서 받은 개인의 TOKEN 을 활용해 url 을 먼저, formating을 해준다. requests의 get 메소드를 활용해 url 의 정보를 받아온다. 받아온 정보를 requests 의 json 메소드로 json 형식으로 변환해준다. json 을 확인하고 원하는 정보를 return 해준다.12345def forecast(lat, lng): url = \"https://api.darksky.net/forecast/&#123;&#125;/&#123;&#125;,&#123;&#125;\".format(TOKEN, lat, lng) response = requests.get(url) json_obj = response.json() return json_obj[\"hourly\"][\"summary\"] 3. [ html ] BS4 활용 html selecting 해서 가져오기 네이버의 실시간 검색순위 부분의 text 를 크롤링 해보자 html 파일을 BS4 를 활용해 받아온뒤, CSS selecting 으로 원하는 text data 를 가져온다. 123456789101112131415from bs4 import BeautifulSoupdef naver(): url = \"https://www.naver.com\" df = pd.DataFrame(columns=[\"rank\", \"keyword\"]) response = requests.get(url) dom = BeautifulSoup(response.content, \"html.parser\") # BeautifulSoup(markup, features, builder, parse_only, from_encoding, exclude_encodings) for keyword in keywords: df.loc[len(df)] = &#123; \"rank\":keyword.select_one('.ah_r').text, \"keyword\":keyword.select_one('.ah_k').text &#125; return df print(response.content)의 모양을 보자. 따라서, BeautifulSoup 으로 html 형식으로 parsing 해주고, css 를 활용해 selecting 해준다. .select 는 여러개의 엘리먼트를 선택 -&gt; 결과가 리스트 .select_one은 하나의 엘리먼트를 선택 -&gt; 결과가 하나의 객체","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"}],"tags":[{"name":"Crawling, Web, Requests","slug":"Crawling-Web-Requests","permalink":"https://emjayahn.github.io/tags/Crawling-Web-Requests/"}]},{"title":"Selenium","slug":"Selenium","date":"2018-10-24T03:02:50.000Z","updated":"2018-10-24T14:54:10.426Z","comments":true,"path":"2018/10/24/Selenium/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/Selenium/","excerpt":"","text":"Selenium 자동화 할 수 있는 프로그램 [ Install Selenium ] chrome driver 다운로드 1$ mv ~/Download/chromedriver /usr/local/bin Selenium Python Package 설치 1$ sudo pip3 install selenium 1. 셀레니움 사용해보기 import 1from selenium import webdriver Open Browser (Chrome Driver) 1driver = webdriver.Chrome() 페이지 이동 1driver.get(url) 브라우져 상에서 자바 스크립트 실행 1driver.execute_script(\"window.scrollTo(300, 400)\") 지금 control하고 있는 window 객체 확인 123main_window = driver.current_window_handlemain_window#현재 control 하고 있는 window 의 객체를 리턴한다. 열려 있는 전체 window 탭 모두 확인 123windows = driver.window_handleswindows#현재 열려있는 모든 탭의 객체를 리스트 형태로 리턴한다. 열려있는 window 탭 중, control 대상 window 로 바꾸기 123driver.switch_to_window(windows[0])#다시 원래 열었던 창으로 돌아가기driver.switch_to_window(main_window) 2. Alert 와 Confirm 다루기 웹을 자동화해서 다니다보면, Alert 창이나 Confirm 창이 의도치 않게 나올 수 있다. 이를 다루는 방법은 다음과 같다. Alert와 Confirm 창의 차이는, Alert 는 확인 창 하나만 있고, Confirm은 확인과 취소가 같이 있는 창이다. (1) Alert Alert 띄우기 12drive.switch_to_window(main_window)drive.execute_script(\"alert('이게 alert 로 뜰겁니다.');\") Alert 확인 누르기 12alert = driver.switch_to.alertalert.accept() Alert 창이 있으면 확인을 누르고, 없으면 없다고 리턴하기 (예외처리) 12345try: alert = driver.switch_to.alert alert.accept()except: print(\"alert 가 없습니다.\") (2) Confirm Confirm 띄우기 1driver.execute_script(\"confirm('이게 Confirm 창입니다.');\") Confirm 창 확인 누르기 or 취소 누르기 12345confirm = driver.switch_to.alert# 확인# confirm.accept()# 취소confirm.dismiss() 3. 입력창에 글씨 입력하기 이제부터 Selenium을 통해 특정 html 의 element 에 액션을 주려면,각종 Selector 를 사용하여 html 상의 element을 셀렉팅 하고, 해당 element 에게 액션을 주어야한다. 1driver.find_element_by_css_selector(nameof cssselector).send_keys(\"입력할 내용\") .find_element_by_css_selector 와 .find_elements_by_css_selector 는 다르다.element 는 하나의 selector 만 선택하는 반면, elements 는 여러가지 element 를 셀렉팅 해서, 리스트 형식으로 Return 한다. 따라서 이렇게 리스트 형식으로 Return 이 된 경우, List[0] 등과 같이 그 엘리먼트를 뒤에 지정해줘야된다. 즉,1234driver.find_element_by_css_selector(nameof cssselector)#위는 바로 selecting 이 된것이지만,driver.find_elements_by_css_selector(nameof cssselector)[0]#위는 뒤에 selecting 위해 list 의 요소를 선택 해주어야한다. 4. 버튼 클릭하기12# 위의 방법처럼, element 를 선택해준다.driver.find_element_by_css_selector(\"name of css selector\").click() 5. id, class, 그 외의 attribute 값을 selecting 하는 법1234driver.find_element_by_css_selector(\"name of css selector.\")click()# id 의 경우 : #idname# class 의 경우 : .classname# 다른 attribute 의 경우 : [attribute = 'value']","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"}],"tags":[{"name":"Web, Crawling, Selenium","slug":"Web-Crawling-Selenium","permalink":"https://emjayahn.github.io/tags/Web-Crawling-Selenium/"}]},{"title":"Thread","slug":"thread","date":"2018-10-24T03:02:50.000Z","updated":"2018-10-24T14:54:22.422Z","comments":true,"path":"2018/10/24/thread/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/thread/","excerpt":"","text":"Thread 파이썬 기본 : Single Thread (Main Thread) threading 모듈 : main thread에서 subthread 를 생성하여 진행하는 방식 multiprocessing 모듈 : double cpu ThreadPoolExecutor : API - 멀티스레드와 멀티프로세스 동일한 형태로 디자인(Pool 클래스만 변경하면됨) threading.Thread() arguement12345Thread(group=, target=, args= , kwargs=, *, daemon=None)#target= : 실제 스레드에서 돌아가게 될 함수#args= : tuple 로 target 함수에 들어가게될 argument#kwargs= : dictionary로 target 함수에 들어가게될 argument#daemon : 데몬 스레드로 돌아갈지 여부 12345#Thread 의 메소드start(): #스레드의 실행, self 의 run() 메소드를 호출run(): #스레드가 실제로 수행하게될 작업name : #스레드의 이름threading.locals() : #해당 스레드 내부에서 사용할 로컬 변수 지정","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"}],"tags":[{"name":"Thread","slug":"Thread","permalink":"https://emjayahn.github.io/tags/Thread/"}]},{"title":"181024 TodayWhatILearned","slug":"181024-TodayWhatILearned","date":"2018-10-24T01:41:11.714Z","updated":"2018-10-24T14:53:10.975Z","comments":true,"path":"2018/10/24/181024-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/181024-TodayWhatILearned/","excerpt":"","text":"181024 TWIL오늘 할 일은 무엇인가 멀티스레딩으로 네이버 크롤링 코드 작성 오후 4시 스터디 확률 분포 공부 오늘 한 일은 무엇인가 멀티스레딩 개념을 활용한 크롤링 코드 작성 오후 4시 스터디 확률분포 공부 내일 할 일은 무엇인가 확률분포 공부 프로젝트 모임 Pandas 정리해보기 무엇을 느꼈는가 Documentation 과 각종 개념 자료들을 혼자 보고 공부하면서, 파이썬이라는 언어가 단순히 책 한권 끝냈다고 해서 끝나는 언어가 아님을 깨달았다. 기본적인 문법을 금방 익숙해져서, ‘역시 쉬운 언어인가’라고 생각했다가 오늘 다양한 자료를 찾아보고 읽어보면서 언어 하나만해도 아직 공부할게 무궁무진 하다는 것을 깨달았다.","categories":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/categories/TWIL/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"181023 TodayWhatILearned","slug":"181023-TodayWhatILearned","date":"2018-10-23T04:31:39.118Z","updated":"2018-10-24T14:53:07.012Z","comments":true,"path":"2018/10/23/181023-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/23/181023-TodayWhatILearned/","excerpt":"","text":"181023 TWIL오늘 할 일은 무엇인가 Queue 정리 하기 확률 분포 공부 &lt;스터디&gt;멀티 스레딩 개념을 포함한 네이버 크롤링 코드짜기 오늘 한 일은 무엇인가 &lt;스터디&gt; 멀티 스레딩에 관해 이야기, 공부해볼 정보들 공유, 어떻게 코드를 구성할 것인지 의견을 나누었다. Queue 개념 정리 및 코드 구현 연습 베르누이 분포, 이항분포 정리 및 Searching 블로그 수정 내일 할 일은 무엇인가 오후 4시 멀티 스레드 관련한 내용으로 온라인 스터디 모임 예정 멀티스레딩 개념을 활용한 네이버 크롤링 코드 짜기 각종 확률 분포 및 검정 부분까지 복습 무엇을 느꼈는가 오늘 공부한 testing 시작부분부터는 그간 배웠던 것을 한꺼번에 적용한다. 배웠던 여러가지 분포를 활용하여,가설을 세우고 이 가설이 선택한 분포의 관점에서 봤을 떄, 가설을 선택할지 기각할지에 관한 내용은 점차data를 통해 prediction 을 해가는 과정에 있는 듯한 느낌을 받았다. 아직 검정 과정과 앞으로 공부하게 될 여러가지 분석 모델을 이해하기에 앞서배운 확률분포 부분의 내용이부족한듯하여, 계속 꾸준히 앞부분을 공부해야될 것 같다.","categories":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/categories/TWIL/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"181022 TodayWhatILearned","slug":"181022-TodayWhatILearned","date":"2018-10-21T23:22:41.104Z","updated":"2018-10-24T14:53:02.814Z","comments":true,"path":"2018/10/22/181022-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/22/181022-TodayWhatILearned/","excerpt":"","text":"181022 TWIL오늘 할 일은 무엇인가 블로그 테마 바꾸기 Selenium 과제 두번째 것 (17:47 - 19:00) 확률 분포 다시 한번 정리하기 데이터구조 (PseudoLinkedList, Stack, Queue)정리 새 맥북 환경 설정 조금씩 마저 더하기 DailyScrum Upload 오늘 한 일은 무엇인가 Selenium NBA Page Crawling 해서, DataFrame으로 정리 (17:47 - 18:55) Stack 정리 Github 설정 내일 할 일은 무엇인가 확률 분포 공부 Queue 정리 무엇을 느꼈는가 맥북을 바꾸면서, 환경 설정하는데 너무 많은 시간이 든다. 틈틈히 하고 있는데도 아직 이전 맥북의 환경에서 100% 똑같은 환경을 만들지 못하고 있다. 장비를 바꿀 때, 혹은 다른 작업 환경에서 연속성을 이어 나가기 위해 나에게 맞는 환경설정도 정리할 필요성을 느낀다. 오늘 공부한 PseudoLinkedList, Stack, Queue 의 활용성에 대해 깊이 고민 해 볼 수 있어서 뿌듯하다. 특히나 PseudoLinkedList의 경우 내가 사용하지 않았던 자료구조나 class를 목적에 맞게 customize 할 수 있는 측면에서 활용성이 높다고 생각된다. 다른 사람이 작성한 class 를 잘 읽어보고, 그 활용도를 높이기 위해 overriding 하는 방법을 틈틈히 챙겨 봐야겠다.","categories":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/categories/TWIL/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"181019 TodayWhatILearned","slug":"181019_TodayWhatILearned","date":"2018-10-18T15:37:56.000Z","updated":"2018-10-24T14:52:39.547Z","comments":true,"path":"2018/10/19/181019_TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/19/181019_TodayWhatILearned/","excerpt":"","text":"181019오늘 한 일 Web Crawling (Requests, BS4) 공부 Github 사용할 준비 GitPages 사용 방법 공부 했던 내용, 공부할 내용 어떻게 정리 할지 고민 프로젝트 팀 회의 내일 할 일 Pandas 내용 정리, 반복 숙달 수업 내용 복습 주말 공부할 내용 계획 코딩 습관을 기르기 위해 일일코딩과 TWIL에 대해 고민 뭘 느꼈는가 크롤링 코드를 보면서 공부하고, 또 다시 스스로 짜는 것을 연습하면서 내용의 개념과 코드 자체는 어려운 것이 없으나 BeautifulSoup 이나 Requests 패키지에 담겨있는 각 메서드 들의 종류, 기능, 메서드마다 들어가는 attribute를 참고 할 줄 알아야 된다고 느꼈다. 매우 기본적인 메서드들일 수 있기에 외워야 할 수도 있겠지만 (또한 반복한다면 숙달되겠지만) 잊어버렸을 때 각 라이브러리들의 Document들을 잘 볼 줄 아는 것이 중요하다고 느꼈다. 프로젝트 OT 를 한 만큼 본격적인 프로젝트를 진행하기에 앞서 꼼꼼히 준비해 나가야겠다.","categories":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/categories/TWIL/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]}]}