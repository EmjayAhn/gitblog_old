{"meta":{"title":"Emjay's DailyCommit Blog","subtitle":"Emjay's DataScience & Development","description":"매일 공부한 것, 알게된 것을 기록하는 블로그입니다. 또한 제가 알고 있는 지식을 찾아보기 위해 개인 위키처럼 사용하고 있습니다.","author":"EmjayAhn(Minjae Ahn)","url":"https://emjayahn.github.io"},"pages":[],"posts":[{"title":"181119-TodayWhatILearned","slug":"181119-TodayWhatILearned","date":"2018-11-19T00:29:31.000Z","updated":"2018-11-19T13:48:49.367Z","comments":true,"path":"2018/11/19/181119-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/19/181119-TodayWhatILearned/","excerpt":"","text":"181119 TWIL 오늘 한 일은 무엇인가 시계열 공부 MA, AR, ARMA 공부 및 수식 꼼꼼히 전개 스터디 모임 : 프로그래밍 구현계획짜기 내일 할 일은 무엇인가 ARIMA, SARIMA 수식 정리 DataStructure 코드 정리 A* 알고리즘 공부 회기분석 리뷰 무엇을 느꼈는가 할게 산더미인듯한 느낌이지만, 하나씩 그어나가다보면 되겠지라는 마음으로…체력적으로 힘듬이 느껴진다..","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181114-TodayWhatILearned","slug":"181114-TodayWhatILearned","date":"2018-11-14T13:01:39.000Z","updated":"2018-11-14T13:05:24.610Z","comments":true,"path":"2018/11/14/181114-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/14/181114-TodayWhatILearned/","excerpt":"","text":"181114 TWIL 오늘 한 일은 무엇인가 flight prediction 내일 할 일은 무엇인가 프로젝트 모임 무엇을 느꼈는가 계속 비행기 delay시간을 예측하는 프로젝트를 진행하고 있다. 여러가지 feature 들을 다루면서 arrival delay 를 예측하는 것을 목표로 삼고 있는데, 가지고 있는 데이터에서 공통적인 특징을 발견할 수가 없다. 외부 데이터를 사용해야할지, 갸지고 있는 데이터 셋에서 새로운 컬럼을 만들어야 할지 모르겠다. 많은 컬럼을 만들고 지우면서, 단순히 아이디어, 혹은 이러지 않을까라는 추측으로 컬럼을 만드는 것이 방법론 적으로 잘못된 것인가하는 의문이 들기도 한다. correlation 이 높은 것을 가지고 OLS 를 돌리는 것 외에 새로운 컬럼을 가지고 만드는 것. 어떻게 다루어야 할지 고민이다. 이제 due date 가 얼마 남지 않은 만큼 최대한 남은 기간 열심히 돌려보고 만들어보면서 계속 trial and error 로 찾아보아야 겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181112-TodayWhatILearned","slug":"181112-TodayWhatILearned","date":"2018-11-12T07:47:46.000Z","updated":"2018-11-12T14:49:02.762Z","comments":true,"path":"2018/11/12/181112-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/12/181112-TodayWhatILearned/","excerpt":"","text":"181112 TWIL 오늘 한 일은 무엇인가 프로젝트 발표 및 Feedback 내일 할 일은 무엇인가 프로젝트 모임 &amp; 시계열 공부 무엇을 느꼈는가 메인프로젝트를 다루는 기간동안의 중간결과를 발표하는 시간을 가졌다. 전달력이 충분하지 못한 느낌을 많이 받았다. 내 머릿속에 있는 것을 다른 사람에게 전달하는 능력 역시 많이 필요함을 깨달은 시간이었다. 프로젝트 내용에 있어서는, 아직 부족한것이 많다. 퍼포먼스도 눈에 띄게 나오는 것이 없다. 원인은 아마도 눈에 띄는 데이터들의 경향성을 보지 못한 탓이 아닐까 싶다. 가지고 있는 train 데이터 전체를 두고는 뚜렷한 경향성을 보지 못하고 있다. 여러가지 제한 조건을 두면서 쪼개서 봐야하는 작업이 더 필요한 것 같다.","categories":[],"tags":[]},{"title":"181111-TodayWhatILearned","slug":"181111-TodayWhatILearned","date":"2018-11-11T14:14:47.000Z","updated":"2018-11-12T07:47:55.277Z","comments":true,"path":"2018/11/11/181111-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/11/181111-TodayWhatILearned/","excerpt":"","text":"181111 TWIL 오늘 한 일은 무엇인가 Project 모임 내일 할 일은 무엇인가 Project_Data 탐색 시계열 분석 공부 무엇을 느꼈는가 어제보다는 약간의 진보가 있었지만, 이에 대한 이유는 명확히 몰라 사실 분석이라기보다 얻어걸린 기분이든다. EDA 를 통해 작성한 모델링에서의 식은 아직까지 완성된 느낌을 받지 못해 답답하고, 방향성을 잘못 접근하고 있는 것인가 하는 느낌을 받기도 했지만, 마지막에 조금 기분이 나아졌다. 내일 오전에는 프로젝트에 밀렸던 시계열 데이터 분석에 관한 기초적인 공부를 다시 하고, 오후에는 프로젝트 데이터를 좀더 살펴보아야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181110-TodayWhatILearned","slug":"181110-TodayWhatILearned","date":"2018-11-10T13:20:02.000Z","updated":"2018-11-10T13:24:15.964Z","comments":true,"path":"2018/11/10/181110-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/10/181110-TodayWhatILearned/","excerpt":"","text":"181110 TWIL 오늘 한 일은 무엇인가 Prediction main project data 탐색 내일 할 일은 무엇인가 Project 모임 무엇을 느꼈는가 어제 고민했던, 시간을 사용하여 Regression 을 돌리는데는 성공하였다. 단위를 바꿔주는 방법으로 60진법도 생각하고, radian 으로 바꾸는 방법도 생각해보다가 epoch 방법으로 기준시점에서 지난 초 수 로 scale 과 unit 을 바꿔준디 돌렸더니, 돌아는간다… 문제는 전혀 예측했던 모형이 아니었다. scatter plot 의 외형만 보고 판단했다는 것을 마지막에 깨달은 것 같다. 모델을 하기위해 묶어보고 새로운 feature 라고 생각되는 것도 빼보지만 아직 경향성이나 공통적인 특정 같은 것은 보이지 않는다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181109-TodayWhatILearned","slug":"181109-TodayWhatILearned","date":"2018-11-09T14:14:01.000Z","updated":"2018-11-09T14:23:02.860Z","comments":true,"path":"2018/11/09/181109-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/09/181109-TodayWhatILearned/","excerpt":"","text":"181109 TWIL 오늘 한 일은 무엇인가 Crawling miniproject 발표 Prediction main project data 탐색 내일 할 일은 무엇인가 Project 모임 무엇을 느꼈는가 Crawling miniproject 의 troubleshooting 시간이 있었다. 발표를 끝내고 나서는 하나를 마쳤다는 시원한 감이 있었지만, comment 들을 듣고 많이 부족함을 느낀 시간이었다. 코드를 작성할 때, 공통된 요소의 추상화가 선택적이 아니라 필수적임을 알게되었다. 간단한 것이라도 반복적으로 작성하는 것은 python convention 에도, 또 프로그래밍의 근본적인 취지에도 어긋나는 것이다. 처음부터 일반화된 포맷으로 작성하기엔 아직 실력이 많이 부족하다. 간단한 구현에서 module 화까지 내가 짜는 대부분의 코드를 연습하다 보면 늘겠지… 메인 프로젝트의 data 간의 연관성이 잘 보이지 않는다. 지난 시간에 알게된 하루 주기로의 delay가 반복됨을 알고 있었으나, datetime 형식으로, 혹은 int나 float 형식으로 ols 에 집어 넣으면 주기성을 잡아내지 못하는 것 같다. 시간 단위로 X feature 로 들어가면 좋을 것 같은데, ols formula 에 시간을 집어넣으면 계속 category 화 된다. 이렇기 때문에 그 해당 정확한 시간이 있지 않으면 coefficient 가 먹지 않지…. 시간을 표현하기 위해 60진법도 찾아보고, 1분 단위로 int 숫자에 mapping 할까도 생각해보았으나, 결국 mapping 해서 ols 에 돌리면 2400 이후 값은 없는데도 x 축에 들어가게 된다….","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181108-TodayWhatILearned","slug":"181108-TodayWhatILearned","date":"2018-11-08T13:18:33.000Z","updated":"2018-11-08T13:20:45.978Z","comments":true,"path":"2018/11/08/181108-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/08/181108-TodayWhatILearned/","excerpt":"","text":"181108 TWIL 오늘 한 일은 무엇인가 Project Data EDA, OLS 돌려보기 내일 할 일은 무엇인가 Project Data EDA, OLS 돌려보기 무엇을 느꼈는가 프로젝트 과정에서 Performance 가 안나오는 이유에 대해 알게된 계기였다. EDA 는 계속 하더라도 부족함이 많은것이고,데이터를 처리할 때 line by line 근거가 있어야 한다고 느꼈다. 처음부터 전 과정을 진행하는데 있어 매우 시행착오가 많았고, 또 앞으로도 많을 것이지만 계속 반복해서 시행해보는 것이 중요할 것 같다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181107-TodayWhatILearned","slug":"181107-TodayWhatILearned","date":"2018-11-07T10:42:45.000Z","updated":"2018-11-08T11:19:43.338Z","comments":true,"path":"2018/11/07/181107-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/07/181107-TodayWhatILearned/","excerpt":"","text":"181107 TWIL 오늘 한 일은 무엇인가 Crawling Project 정리 및 제출 내일 할 일은 무엇인가 Prediction Project","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181105-TodayWhatILearned","slug":"181105-TodayWhatILearned","date":"2018-11-05T03:15:36.000Z","updated":"2018-11-05T13:35:12.039Z","comments":true,"path":"2018/11/05/181105-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/05/181105-TodayWhatILearned/","excerpt":"","text":"181105 TWIL 프로젝트 STEP 전처리 Nan 값 처리 해결 Partial Regression Plot 그려보기 오늘 한 일은 무엇인가 프로젝트 모임 Crawling miniProject 와 mainProject 진행 내일 할 일은 무엇인가 프로젝트, 오늘까지 진행사항 코드 정리 수학 회기 전체 정리 프로젝트 생각하기 무엇을 느꼈는가 오늘은 프로젝트 모임을 통해서, 많은 것을 얻었다. OLS function 을 실제로 돌림에 있어서 많은 제약조건이필요함을 알게되었고, 그만큼 데이터전처리가 매우 중요하다는 것을 알게 되었다. 다양한 feature 들마다 다양한 방법으로 전처리를 해주어야 한다. 처음엔 NaN값의 처리 방식에 대해 고민했으나,이제는 각 feature 의 특성들마다 전처리 해주는 방식이 달라져야 하고, 또 이번 고비가 넘어가게 되면좋은 prediction 결과를 얻기 위해 다양한 feature 의 조합이 필요함이 피부에 와닿았다. 만족스런 결과물을 얻기 위해선, 아는게 많은 것 보다, 그 결과물을 만들고자 하는 구성원이 중요함을 느꼈다.알고있는 지식은 해결해야할 문제보다 항상 작기 마련이다. 또한 알고있는 지식이 완벽한지는 계속 스스로 의문을던지며 업데이트 해야만한다. 하지만 이보다 더 중요한 것은 문제에 부딪힐 때마다 의욕적이고, 해결해보고자 하는 팀원들덕분에 오늘의 보람과 뿌듯함을 얻을 수 있었던 것 같다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181104-TodayWhatILearned","slug":"181104-TodayWhatILearned","date":"2018-11-04T14:07:51.000Z","updated":"2018-11-04T14:10:26.054Z","comments":true,"path":"2018/11/04/181104-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/04/181104-TodayWhatILearned/","excerpt":"","text":"181104 TWIL 오늘 한 일은 무엇인가 회기 부분 공부 Project Data 회기 돌려보기 내일 할 일은 무엇인가 Project 모임 회기 부분 정리 크롤링 프로젝트 개선 사항 고민 (시간이 되면) A* 알고리즘 코드 짜보기 무엇을 느꼈는가 다리를 다치고 통증으로 인해 집중해서 코드를 짜기 힘들었다. 수학 이론을 공부하면서 내일 있을 프로젝트모임을 준비했고, 팀원들께서 만든 코드를 정리하며 데이터를 다시 추출하였다. 이 데이터들을 통해 회기의 몇몇가지를 돌려보았다. 이것을 토대로 내일 팀원들과 함께 이야기를 나눠보며 좀더 어떻게 할 수 있을지 고민해봐야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181103-TodayWhatILearned","slug":"181103-TodayWhatILearned","date":"2018-11-03T13:27:46.000Z","updated":"2018-11-03T13:39:37.686Z","comments":true,"path":"2018/11/03/181103-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/03/181103-TodayWhatILearned/","excerpt":"","text":"181103 TWIL 오늘 한 일은 무엇인가 Crawling miniProject 코드 작성 Pandas 라이브러리 정리 Project 주말동안 할일 정하기 내일 할 일은 무엇인가 Project Data 회기 돌려보기 회기 부분 공부 크롤링 프로젝트 개선 사항 고민 (시간이 되면) A* 알고리즘 코드 짜보기 무엇을 느꼈는가 whoscored 사이트의 Player 정보를 크롤링 하는 miniProject 의 코드를 작성하였다. Selenium을통해서 크롤링하는데, headless 적용을 하면 에러가 나는 부분을 어떻게 처리해야할지 고민해보아야 한다.Chrome webdriver 를 열어놓는 환경과 headless 환경의 차이가 있는 것 같은데, 이 부분은 document를 살펴보아야 할 것같다. window창을 열어놓는것 과 그렇지 않은 것의 차이점이 있는지 확인해야한다. 함수로 짠 코드를 class 화 시킬 때는 다루는 범위가 커져야 한다는 강박관념이 있다. class 화 시켰을때의 편의성 부분을 고민하면서 위 생각으로 흐름이 이어지는 것 같은데, 그렇지 않기 위해 class의 장점을좀더 체감해볼 필요가 있다. Linear Regression 강의를 들으면서, LineByLine 수식을 이해하는데는 문제가 없으나 이야기의 큰 그림을 놓치는 경향이 있는 것 같다. 내일은 이 부분을 중점적으로 공부해보고, 메인 프로젝트에 적용해보는 것 까지해봐야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181102-TodayWhatILearned","slug":"181102-TodayWhatILearned","date":"2018-11-02T12:48:52.000Z","updated":"2018-11-02T13:01:45.304Z","comments":true,"path":"2018/11/02/181102-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/02/181102-TodayWhatILearned/","excerpt":"","text":"181102 TWIL 오늘 한 일은 무엇인가 Database - MySQL 공부 A * 알고리즘 스터디 NaN 값 처리에관한 자료 서칭 내일 할 일은 무엇인가 Project 모임 Pandas 라이브러리 정리 miniProject Crawling 코드짜기 A * 알고리즘 짜기 무엇을 느꼈는가 오늘 스터디에서 A* 알고리즘를 주제로 얘기를 좀더 나누었다. 각자 공부해오신 내용을 바탕으로 알고리즘의흐름이 어떻게 되어가는가 좀더 구체적으로 생각해보고자 했다. 좀더 해결법에 가까워진 느낌을 받았으나,이제는 좀더 구체적으로 구현해보면서 다가오는 문제들을 해결해보고자 했다. 모든걸 완벽하게 이해하고 실현하는것만이 답은 아니기 때문이라고 생각한다. 주말동안은 프로젝트, 미니프로젝트, 알고리즘 적용 코딩, 수학 공부, DB공부.. 산더미지만 하나씩 그어나가야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181101-TodayWhatILearned","slug":"181101-TodayWhatILearned","date":"2018-11-01T14:10:02.000Z","updated":"2018-11-01T14:20:02.441Z","comments":true,"path":"2018/11/01/181101-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/11/01/181101-TodayWhatILearned/","excerpt":"","text":"181101 TWIL 오늘 한 일은 무엇인가 Project 모임 Data 일부 전처리 Crawling 공부 내일 할 일은 무엇인가 Database - MySQL 공부 Pandas 라이브러리 살펴보기 miniProject Crawling 주제 선정, 코드짜기 무엇을 느꼈는가 프로젝트 모임에서 우리가 예측할 Data (Departure Delay)를 정의하고, 나는 오늘 시각 data 의 전처리를 하였다. Data 들이 의미없는 값들을 가지고 있을 때 어떻게 처리해야 할지 고민하다가 확실한 답은 못얻은 채, 우선 시각 데이터의 formatting 만 바꾸었다. data들을 어떻게 채워넣어주어야 할지는 조금더 공부해 보아야 할 것 같다. 첫 프로젝트 모임의 느낌이 매우 좋았다. 모임을 하기 전까지는 내가 했던 것들이 맞는가 하는 의구심과 project를 하기에는 아직 부족한 지식과 실력이라는 걱정이 앞섰다. 오늘 모임을 하면서 각자 살펴보았던 data 의 특징들과 앞으로 어떻게 data 를 다듬을 것인지 얘기하면서, 더 좋은 방향과 몰랐던 것들, 알았던 것들을 서로 나누면서 발전적인 대화가 되었다는게 매우 뿌듯하다. 모임에서 받은 좋은 느낌을 이어서, 오늘 내가 맡기로한 부분을 해결하기 위해 앉았고, 또 나름 해결한 부분이 있는 것 같아 작은 성취감을 맛보았다. 협업을 하기 위해 git에 관해 좀더 공부하고 나눌 필요성이 느껴졌다. 특히나 code conflict 가 실제로 발생하기 전에 어떻게 다루어야 하는 것인지 좀더 깊게 공부할 필요성을 느꼈다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"miniproject","slug":"miniproject","date":"2018-11-01T07:04:30.000Z","updated":"2018-11-01T07:04:30.613Z","comments":true,"path":"2018/11/01/miniproject/","link":"","permalink":"https://emjayahn.github.io/2018/11/01/miniproject/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Mysql","slug":"MySQL","date":"2018-11-01T03:20:44.000Z","updated":"2018-11-06T01:17:51.764Z","comments":true,"path":"2018/11/01/MySQL/","link":"","permalink":"https://emjayahn.github.io/2018/11/01/MySQL/","excerpt":"","text":"MySQL1. Install MySQL https://dev.mysql.com/downloads/mysql/5.7.html#downloads 에서DMG 파일 다운로드 시스템 환경설정에서 MySQL -&gt; Start MySQL Server : server 시작 1$ cd usr/local/mysql/bin 위 경로에서 MySQL 서버에 접속1$ sudo ./mysql -p 1mysql &gt; 패스워드 변경123mysql&gt;ALTER USER 'root'@'localhost' IDENTIFIED BY '바꾸고싶은 비밀번호';mysql&gt;FLUSH PRIVILEGES;mysql&gt;quit; 2. MySQL Shell Command(1) DATABASE 생성, 접속, 삭제 현재 상태 보기 1mysql&gt; STATUS DB 목록 보기 1mysql&gt; SHOW DATABASES; DB 만들기 1mysql&gt; CREATE DATABASE DBNAME DB 접속하기 1mysql&gt; USE DBNAME; 현재 접속중인 DB 확인하기 1mysql&gt; SELECT DATABASE(); DB 지우기 1mysql&gt; DROP DATABASE DBNAME; (2) TABLE 생성, 추가, 삭제 table 만들기 1234567CREATE TABLE table_name( column_name_1 column_data_type_1 column_constraint_1, column_name_2 column_data_type_2 column_constraint_2, . . .) column_constraint 는 Optional 이다. (unique 와 같은 제약조건) user 라는 table에 name, email, age 컬럼 생성 example1 : constraint 가 없을 떄, 12345mysql&gt; CREATE TABLE user( name CHAR(20), email CHAR(40), age INT(3)) example2 : constraint가 있을 때, 1234567mysql&gt; CREATE TABLE user2( user_id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(20) NOT NULL, email VARCHAR(30) UNIQUE NOT NULL, age INT(3) DEFAULT'30', rdate TIMESTAMP) (3) 수정(3)-1. DATABASE 수정 DATABASE 의 encoding 수정 현재 문자열 encoding 확인 1mysql&gt; SHOW VARIABLES LIKE \"CHARACTER_SET_DATABASE\"; mydb 데이터베이스의 문자열 인코딩을 utf8 로 변경 1mysql&gt; ALTER DATABASE mydb CHARACTER_SET = utf8; user 데이터베이스의 문자열 인코딩을 ascii 로 변경 1mysql&gt; ALTER DATABASE user CHARACTER_SET=ascii (3)-2. TABLE 수정 user 테이블에 tmp라는 컬럼명, TEXT 데이터 타입 컬럼을 추가1mysql&gt; ALTER TABLE user ADD tmp TEXT;","categories":[{"name":"Database","slug":"Database","permalink":"https://emjayahn.github.io/categories/Database/"}],"tags":[]},{"title":"181031-TodayWhatILearned","slug":"181031-TodayWhatILearned","date":"2018-10-31T14:17:59.000Z","updated":"2018-10-31T14:27:39.155Z","comments":true,"path":"2018/10/31/181031-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/31/181031-TodayWhatILearned/","excerpt":"","text":"181031 TWIL 오늘 한 일은 무엇인가 선형회귀분석 공부 행렬의 미분 공부 (PROJECTmini)A star Algorithm 개념 정리 (PROJECT)프로젝트 진행 data 탐구 데이터 전처리 공부 내일 할 일은 무엇인가 Project 모임 데이터 전처리, 데이터들의 의미 파악, 예측할 delay 부분 정의하기 무엇을 느꼈는가 오늘은 아침부터 하루종일 컴퓨터 앞에 앉아 있어서, 눈과 목이 너무 아프다. 그래도 새로운 알고리즘 내용에관한 이해와, 앞으로 진행할 메인 프로젝트의 data들의 의미를 파악해서 뿌듯한 하루였다. Data의 의미를파악했으나 이것을 처리하기 위해서 numpy 와 pandas 의 documentation 을 읽으면서 데이터들을 다뤘다.라이브러리와 패키지들의 메소드와 클래스들을 자유자재로 다루기 위해선, 계속 사용해보면서 익혀야 함을 느꼈다.메소드들을 알면 복잡하게 코드를 안짜도 이미 내장되어있는 메소드로 손쉽게 처리할 수 있기 때문이다. data를 혼자 곰곰히 보다, 시간에 관한 column 의 의미를 파악했으나 이것을 손쉽게 합쳐서 데이터들을재정렬하는데 오늘 실패했다. 의미는 파악했으니, 내일 좀더 시도해보면 시간에 관한 data 를 정리할 수 있지않을까 한다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181030-TodayWhatILearned","slug":"181030-TodayWhatILearned","date":"2018-10-30T14:20:31.000Z","updated":"2018-10-30T14:26:50.227Z","comments":true,"path":"2018/10/30/181030-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/30/181030-TodayWhatILearned/","excerpt":"","text":"181030 TWIL 오늘 한 일은 무엇인가 검정, 추정 부분 수식 꼼꼼히 다시 보기 A star(A*) Algorithm 개념 읽기 내일 할 일은 무엇인가 선형회귀분석 정리 (패키지 별로 특징, 메서드 파라미터 위주) 데이터 전처리 부분 공부 Test data 처리는 어떻게 하는 건지 공부해보기 무엇을 느꼈는가 추정부분 수식을 공부하면서 eigenvalue 부분이 기억이 잘 안나던 것을 시간이 될 때 자료들을 챙겨봐야겠다. 오늘 공부한 선형 회귀분석을 배우니 조금이나마 프로젝트를 어떻게 진행해야되는 건지 감이 잡힌 것 같다. 미약한시작을 하기위해 오늘 공부한 개념들을 사용해서 여러가지 돌려보고 데이터를 파악해볼 수 있을 것 같다.하지만, 우리가 가지고 있는 데이터셋이 바로 적용 할 수 없는 현실적인 데이터이기 때문에, 여러가지 전처리작업이 필요할 것 같다. 오늘 배운 것을 적용해보기 위해 내일은 데이터 전처리를 공부해보고, 또한 test data의생성 역시 공부해보아야할 부분인 것 같다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181029-TodayWhatILearned","slug":"181029-TodayWhatILearned","date":"2018-10-29T14:24:56.000Z","updated":"2018-10-29T14:31:15.814Z","comments":true,"path":"2018/10/29/181029-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/29/181029-TodayWhatILearned/","excerpt":"","text":"181029 TWIL 오늘 한 일은 무엇인가 자료구조 Binary Tree, Stack 으로 Queue구현, Stack 응용 추정 부분 복습 스터디 나갈 방향 이야기 A star algorithm search 내일 할 일은 무엇인가 ‘추정’에서 수식 부분 다시 보기 무엇을 느꼈는가 스터디 첫 모임을 가졌다. 프로그래밍에 관해 알고있는 것들을 활용하고 응용하는 쪽으로 어떻게 하면 될지 많은 의견을 나누었다. 상상의 나래를 펼치니, 타오르는 열정과 함께 만들어보고 싶은 것은 많았으나…. 아직 아는게 많지 않기에.. 다시 현실에 눈을 돌렸다. Maze 문제에서 다른 알고리즘을 통해 구현해보려고 의견이 모아졌다. 직접 찾아서 적용해보는 첫 algorithm 이기에, 직접 찾아가면서 공부하고, 이것을 적용하는 경험을 통해 또 다른 배울 것이 있을 것 같아 매우 기대된다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181028-TodayWhatILearned","slug":"181028-TodayWhatILearned","date":"2018-10-28T13:09:13.000Z","updated":"2018-10-29T14:25:13.521Z","comments":true,"path":"2018/10/28/181028-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/28/181028-TodayWhatILearned/","excerpt":"","text":"181028 TWIL 오늘 한 일은 무엇인가 검정과 추정 공부 Blog 테마 수정 내일 할 일은 무엇인가 ‘추정’에서 수식 부분 다시 보기 (Study) 데이터 구조 강의 듣기 무엇을 느꼈는가 MaximumLikelihood 를 실제로 손으로 써가며 풀어보는 과정에서, 지금까지 배웠던 수학적 테크닉들이 모두 쓰이는 것을 보고 뿌듯하면서도 재미있었다. 뿌듯한 이유는 아마도 한줄 한줄 써가는 내용이 여태 공부한 수학 개념들로 이루어진것 때문일 것이다. 그 중에서도 다음 식을 전개하는 과정에서 눈에 잘 들어오지 않아 전개하지 못할 때도 있었다. 이것은 아마 앞부분 개념이 그 순간에 적용이 되지 않기 때문이라고 생각된다. 행렬의 내용중 몇가지 특성들과 라그랑주 멀티플라이어에 대한 수식을 틈이 생길떄 챙겨서 봐야겠다. 오늘은 수학을 공부하느라 프로그래밍은 하지 못했다. 중간에 졸린 걸 해소해보고자 블로그 테마 색깔 수정과 그 수정을 위해 블로그의 코드 구조를 본게 전부 였다. 30분 푹빠져서 하다가 주객전도 되지않으려 다음으로 미뤘다. 위 주제들에 관해 알게된 것도 많은 하루였고, 뿌듯한 하루였다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"181027-TodayWhatILearned","slug":"181027-TodayWhatILearned","date":"2018-10-27T14:38:04.000Z","updated":"2018-10-28T13:09:52.766Z","comments":true,"path":"2018/10/27/181027-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/27/181027-TodayWhatILearned/","excerpt":"","text":"181027 TWIL오늘 한 일은 무엇인가 확률 수학 공부 자기전 (스터디)데이터구조 부분 1강 듣기내일 할 일은 무엇인가 확률분포를 다시 복습하면서 검정, 추정에서 이어지는 부분 꼼꼼히 공부 데이터 구조 강의 수강 계획 세우기 무엇을 느꼈는가 검정방법론에 대해 좀더 꼼꼼히 보았다. 수학적인 수식들은 수업시간에 다 이해가 되는 편이지만 이 수식들을말로써 표현하고, 글로 풀어쓰는 순간 머리가 빠릿빠릿 안돌아가는 느낌이어서, 한단계한단계 논리적으로 따져가며공부하니 이제야 좀 편해진것 같다. 가설 검정 같은 때에도, 말로 풀어쓰는 것 보다 간단하게 수식으로 표현하고,각각의 p-value 를 확인한뒤 원래 작성했던 H_0, H_a 에 대해 생각해보면 쉽게 되었으나 이것을 말로 표현하고글로 구성하려고 하니 간단한것도 복잡하게 생각했던 것 같다. 결국 내가 알게 된것을 상대방과 논의하고앞으로 만나게 될 클라이언트들을 대상으로 설명해야하는 것이 모두 이런 부분에서 시작되는 것임을 느꼈기에,내 생각과 가정 -&gt; 수식으로 표현 -&gt; 다시 말 혹은 글로 표현 하는 것을 습관처럼 해야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"Scrapy","slug":"Scrapy","date":"2018-10-26T14:04:20.000Z","updated":"2018-10-26T14:05:46.026Z","comments":true,"path":"2018/10/26/Scrapy/","link":"","permalink":"https://emjayahn.github.io/2018/10/26/Scrapy/","excerpt":"","text":"Scrapy 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Scrapy 라이브러리는 파이썬에서 제공하는 라이브러리로써, 대량의 페이지들의 Crawling을 손쉽게 해주는 라이브러리이다. 1. Install 파이썬의 라이브러리 이므로 pip 으로 설치 할 수 있다.1pip3 install scrapy 2. 실습 실습을 위해 import 할 것들 123import scrapyimport requestsfrom scrapy.http import TextResponse requests 를 통해 url 정보를 받아온다. TextResponse 를 통해 받아온 html 파일을 encoding 과 text형식으로 return 12req = requests.get(\"url_name\")response = TextResponse(req.url, body=req.text, encoding=\"utf-8\") 123456a = response.xpath('xpath')# xpath 로 지정한 엘리먼트를 가져온다.a_text = reponse.xpath('xpath/text()')# 엘리먼트의 text data 를 가져온다.a_text.extract()# 엘리먼트의 text data들을 말그대로 extract 하여, list 형태로 return 해준다 3. Scrapy 사용하기(1) scrapy 프로젝트 생성 shell command1scrapy startproject crawler 1!scrapy startproject crawler New Scrapy project &apos;crawler&apos;, using template directory &apos;/Users/emjayahn/.pyenv/versions/3.7.0/envs/dss/lib/python3.7/site-packages/scrapy/templates/project&apos;, created in: /Users/emjayahn/Dev/DSS/TIL(markdown)/crawler You can start your first spider with: cd crawler scrapy genspider example example.com 1!tree crawler crawler ├── crawler │ ├── __init__.py │ ├── __pycache__ │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ └── __pycache__ └── scrapy.cfg 4 directories, 7 files (2) Scrapy 기본 구조 Spider 크롤링 절차 정하기 어떤 웹사이트들을 어떻게 크롤링 할 것인지 선언 각각의 웹페이지의 어떤 부분을 스크래핑 할 것 인지 명시하는 클래스 items.py spider 가 크롤링한 data 들을 저장할 때, 사용자 정의 자료구조 클래스 MVC : 중 Model 부분에 해당 Feature 라고 생각 pipeline.py 스크래핑한 데이터를 어떻게 처리할지 정의 데이터에 한글이 포함되어 있을 때는 encoding=’utf-8’ utf-8인코딩이 필요 settings.py Spider, item, pipeline 의 세부 사항을 설정 (예) 크롤링 빈도 등 (예) robots.txt - ROBOTSTXT_OBEY=True","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"},{"name":"Crawling","slug":"Python/Crawling","permalink":"https://emjayahn.github.io/categories/Python/Crawling/"}],"tags":[]},{"title":"Xpath","slug":"Xpath","date":"2018-10-26T13:02:50.000Z","updated":"2018-10-26T14:05:44.006Z","comments":true,"path":"2018/10/26/Xpath/","link":"","permalink":"https://emjayahn.github.io/2018/10/26/Xpath/","excerpt":"","text":"xpath 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 1. xpath란? xpath = XML Path Language XML 문서의 element나 attribute의 value에 접근하기 위한 언어 XML 문서를 해석하기 위한 언어이므로, 기본적으로 path expression 이 바탕이 되어있다. 연산, 문자열 처리를 위한 라이브러리를 내장하고 있다. 2. Location Path element 를 찾으러 가는 것이므로, location을 나타내기 위한 연산자는 다음과 같다. element_name : element_name 과 일치하는 모든 element를 선택한다. / : 가장 처음 쓰는 /는 절대경로를 의미한다. Path 중간에 쓰는 /는 조건에 맞는 바로 다음 하위 엘리먼트를 검색한다. (css selector에서 &gt;와 같다) // : 가장 상위 엘리먼트 . : 현재 엘리먼트 * : 조건에 맞는 전체 하위 엘리먼트를 검색한다(css selector 에서 한칸 띄우는 것 과 같다) element[조건] : 엘리먼트에 조건에 해당되는 것을 검색한다 (예) p[2] : p 엘리먼트 중 두번째 엘리먼트를 선택 주의:1부터 시작, 0이 아님 (예) [@(attribute_key=&quot;attribute_value&quot;)] : 속성값으로 엘리먼트를 선택[@id=&quot;main&quot;] : “main” 이라는 id 를 선택[@class=&quot;pp&quot;] : “pp” 라는 class 를 선택 not(조건) : 조건이 아닌 엘리먼트를 찾는다. /text() : 해당 엘리먼트의 text 데이터를 가져온다 /@attribute_name : 해당 엘리먼트의 attribute_name 에 할당된 value 값을 가져옴 /@href : 해당 엘리먼트의 href의 value 값을 가져옴","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"},{"name":"Crawling","slug":"Python/Crawling","permalink":"https://emjayahn.github.io/categories/Python/Crawling/"}],"tags":[]},{"title":"181026_TodayWhatILearned","slug":"181026-TodayWhatILearned","date":"2018-10-26T03:49:13.000Z","updated":"2018-10-26T14:46:49.969Z","comments":true,"path":"2018/10/26/181026-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/26/181026-TodayWhatILearned/","excerpt":"","text":"181026 TWIL오늘 한 일은 무엇인가 Xpath, Scrapy 를 활용한 Crawling 공부 DataScience 와 DataEngineering 의 차이점 찾아보기 내일 할 일은 무엇인가 확률분포 공부 Scrapy 활용해보기 무엇을 느꼈는가 DataScience 와 DataEngineering 의 공통점과 차이점에 대해 알아보았는데, 아직은 현업에서 각분야가 하고 있는 일이 어떻게 다른지 확실한 감이 오지 않는 것 같다. 내가 명확히 재밌어하는 것은 아직까지는어떤 디테일한 분야가 아니라, 수학적인 것, 프로그래밍 적인 것들을 배운 것을 적용해보고 응용하는 것에흥미를 느끼는 것 같은데 이보다 더 앞서 생각해보고 결정하려고 하니 감이 잘 오지 않는 것 같다. 지금 현재로서는, 다양한 기계학습 알고리즘과 머신러닝 등을 이용하여 prediction 등을 통해 새로운 Insight를 얻어내는 것에 관심이 있는 것 같다. 이 분야를 공부하고 배워 나가면서 엔지니어링 분야를 필요에 의해 점차공부해 나가는 방향으로 삼고 싶다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"Pillow","slug":"Pillow","date":"2018-10-25T12:22:20.000Z","updated":"2018-10-25T13:16:51.602Z","comments":true,"path":"2018/10/25/Pillow/","link":"","permalink":"https://emjayahn.github.io/2018/10/25/Pillow/","excerpt":"","text":"Pillow 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Pillow 는 Python에서 이미지를 핸들링 하기위한 라이브러리이다. 스크린 샷, 이미지 크롭, 썸네일등을 만들 수 있다. 또한, 다양한 이미지 확장자를 다룰수 있다. (예, jpg, png) Pillow install1pip install Pillow 1. Import Convention Pillow 를 사용하기에 앞서, Import 해야한다.1from PIL import Image 2. Pillow 메소드(1) open() 이름 그대로, 경로와 파일 이름을 적어주면, 해당 이미지리턴하여 Variable 에 할당한다. 1image = Image.open(\"imagefile_name\") (2) crop() 이름 그대로, image 를 잘라준다. 이 때 Parameter 는 box형태로 들어가게 된다. box 는 (left, upper, right, lower) 로 들어가게 된든데, pixel 단위로 들어가면 된다 이미지의 가장 왼쪽과 위쪽 라인을 기준으로, left px 만큼 upper px 만큼 (왼쪽을 기준으로) right px 만큼 (위쪽을 기준으로) bottom px 만큼 잘라, box 로 만들어 준다.123box = (10, 10, 110, 110)image.crop(box)### 이렇게 되면 가로세로 100px 만큼의 이미지가 만들어진다. (3) thumbnail() 썸네일을 만들어준다. 썸네일의 활용 방안은 한 가지 사진을 어플리케이션 곳곳에서 사용한다고 할 때, 데이터 사이즈가 큰 원본 사진을 계속 해서 들고 다니며 사용하면 어플리케이션에 따라 메모리 낭비, 서버용량 낭비, 트래픽 낭비 등으로 이어질 수 있다. 따라서, 이미지의 데이터 크기와 해상도를 낮춰 사용할 수 있다.12image.thumbnail((pixel, pixel))#thumbnail 메소드를 사용하여, 원하는 pixel 수로 줄일 수 있다.","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"}],"tags":[]},{"title":"181025_TodayWhatILearned","slug":"181025-TodayWhatILearned","date":"2018-10-25T00:01:11.000Z","updated":"2018-10-26T04:13:17.230Z","comments":true,"path":"2018/10/25/181025-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/25/181025-TodayWhatILearned/","excerpt":"","text":"181025 TWIL오늘 할 일은 무엇인가 확률분포 공부 프로젝트 모임 Selenium, Scrapy, Pillow 활용 공부 오늘 한 일은 무엇인가 Selenium, Pillow 활용 공부 내일 할 일은 무엇인가 확률분포 공부 무엇을 느꼈는가 동영상과 이미지를 처리하는 방법에 대해 공부하면서, Selenium 을 활용해 많은 활용도를 느꼈다.주말을 이용해 동영상이나 이미지들을 Crawling 해서 class 형태로 만드는 방법을 시도해봐야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"Requests","slug":"Requests","date":"2018-10-24T03:02:50.000Z","updated":"2018-10-26T10:30:57.539Z","comments":true,"path":"2018/10/24/Requests/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/Requests/","excerpt":"","text":"WEB CRAWLING 1 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Requests Requests 패키지는 크롤링 하고자 하는 페이지를 url 값을 통해 가져와 객체로 변환 해주는 기능을 가지고 있다. 1. Installation Requests 를 이용하기 위해 package 를 설치해준다.1$ pip3 install requests request를 이용하면서, json 형식으로의 크롤링과, html 형식으로 css selecting 을 위한 크롤링을 실습해 볼 것이므로, BeautifulSoup Package 역시 같이 설치해준다. (BeautifulSoup은 html parser 를 이용해보기 위함이다.) python-forecastio 는 dark sky 의 api를 활용해 날씨 데이터를 받아올때 사용해보기 위해 설치한다. 12$ pip3 install bs4$ pip3 install python-forecastio import 할 것들 1234import requestsimport forecastiofrom bs4 import BeautifulSoupfrom pandas.io.json import json_normalize 2. [ jSON ] Dark Sky api 활용 날씨정보 가져오기 DarkSky api 는 위도와 경도를 입력하면, 날씨 정보를 주는 api 이다. https://darsky.net/dev/ 에 가입 후, TOKEN 을 받는다. 위에서 받은 개인의 TOKEN 을 활용해 url 을 먼저, formating을 해준다. requests의 get 메소드를 활용해 url 의 정보를 받아온다. 받아온 정보를 requests 의 json 메소드로 json 형식으로 변환해준다. json 을 확인하고 원하는 정보를 return 해준다.12345def forecast(lat, lng): url = \"https://api.darksky.net/forecast/&#123;&#125;/&#123;&#125;,&#123;&#125;\".format(TOKEN, lat, lng) response = requests.get(url) json_obj = response.json() return json_obj[\"hourly\"][\"summary\"] 3. [ html ] BS4 활용 html selecting 해서 가져오기 네이버의 실시간 검색순위 부분의 text 를 크롤링 해보자 html 파일을 BS4 를 활용해 받아온뒤, CSS selecting 으로 원하는 text data 를 가져온다. 123456789101112131415from bs4 import BeautifulSoupdef naver(): url = \"https://www.naver.com\" df = pd.DataFrame(columns=[\"rank\", \"keyword\"]) response = requests.get(url) dom = BeautifulSoup(response.content, \"html.parser\") # BeautifulSoup(markup, features, builder, parse_only, from_encoding, exclude_encodings) for keyword in keywords: df.loc[len(df)] = &#123; \"rank\":keyword.select_one('.ah_r').text, \"keyword\":keyword.select_one('.ah_k').text &#125; return df print(response.content)의 모양을 보자. 따라서, BeautifulSoup 으로 html 형식으로 parsing 해주고, css 를 활용해 selecting 해준다. .select 는 여러개의 엘리먼트를 선택 -&gt; 결과가 리스트 .select_one은 하나의 엘리먼트를 선택 -&gt; 결과가 하나의 객체","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"},{"name":"Crawling","slug":"Python/Crawling","permalink":"https://emjayahn.github.io/categories/Python/Crawling/"}],"tags":[{"name":"Crawling, Web, Requests","slug":"Crawling-Web-Requests","permalink":"https://emjayahn.github.io/tags/Crawling-Web-Requests/"}]},{"title":"Selenium","slug":"Selenium","date":"2018-10-24T03:02:50.000Z","updated":"2018-10-26T14:04:54.585Z","comments":true,"path":"2018/10/24/Selenium/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/Selenium/","excerpt":"","text":"Selenium 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 자동화 할 수 있는 프로그램 [ Install Selenium ] chrome driver 다운로드 1$ mv ~/Download/chromedriver /usr/local/bin Selenium Python Package 설치 1$ sudo pip3 install selenium 1. 셀레니움 사용해보기 import 1from selenium import webdriver Open Browser (Chrome Driver) 1driver = webdriver.Chrome() 페이지 이동 1driver.get(url) 브라우져 상에서 자바 스크립트 실행 1driver.execute_script(\"window.scrollTo(300, 400)\") 지금 control하고 있는 window 객체 확인 123main_window = driver.current_window_handlemain_window#현재 control 하고 있는 window 의 객체를 리턴한다. 열려 있는 전체 window 탭 모두 확인 123windows = driver.window_handleswindows#현재 열려있는 모든 탭의 객체를 리스트 형태로 리턴한다. 열려있는 window 탭 중, control 대상 window 로 바꾸기 123driver.switch_to_window(windows[0])#다시 원래 열었던 창으로 돌아가기driver.switch_to_window(main_window) 2. Alert 와 Confirm 다루기 웹을 자동화해서 다니다보면, Alert 창이나 Confirm 창이 의도치 않게 나올 수 있다. 이를 다루는 방법은 다음과 같다. Alert와 Confirm 창의 차이는, Alert 는 확인 창 하나만 있고, Confirm은 확인과 취소가 같이 있는 창이다. (1) Alert Alert 띄우기 12drive.switch_to_window(main_window)drive.execute_script(\"alert('이게 alert 로 뜰겁니다.');\") Alert 확인 누르기 12alert = driver.switch_to.alertalert.accept() Alert 창이 있으면 확인을 누르고, 없으면 없다고 리턴하기 (예외처리) 12345try: alert = driver.switch_to.alert alert.accept()except: print(\"alert 가 없습니다.\") (2) Confirm Confirm 띄우기 1driver.execute_script(\"confirm('이게 Confirm 창입니다.');\") Confirm 창 확인 누르기 or 취소 누르기 12345confirm = driver.switch_to.alert# 확인# confirm.accept()# 취소confirm.dismiss() 3. 입력창에 글씨 입력하기 이제부터 Selenium을 통해 특정 html 의 element 에 액션을 주려면,각종 Selector 를 사용하여 html 상의 element을 셀렉팅 하고, 해당 element 에게 액션을 주어야한다. 1driver.find_element_by_css_selector(nameof cssselector).send_keys(\"입력할 내용\") .find_element_by_css_selector 와 .find_elements_by_css_selector 는 다르다.element 는 하나의 selector 만 선택하는 반면, elements 는 여러가지 element 를 셀렉팅 해서, 리스트 형식으로 Return 한다. 따라서 이렇게 리스트 형식으로 Return 이 된 경우, List[0] 등과 같이 그 엘리먼트를 뒤에 지정해줘야된다. 즉,1234driver.find_element_by_css_selector(nameof cssselector)#위는 바로 selecting 이 된것이지만,driver.find_elements_by_css_selector(nameof cssselector)[0]#위는 뒤에 selecting 위해 list 의 요소를 선택 해주어야한다. 4. 버튼 클릭하기12# 위의 방법처럼, element 를 선택해준다.driver.find_element_by_css_selector(\"name of css selector\").click() 5. id, class, 그 외의 attribute 값을 selecting 하는 법1234driver.find_element_by_css_selector(\"name of css selector.\").click()# id 의 경우 : #idname# class 의 경우 : .classname# 다른 attribute 의 경우 : [attribute = 'value'] 6. selecting 한 element 의 attribute value 를 얻는 방법 CSS로 선택한 element에 html 태그에는 다양한 attribute 들이 있을 수 있다. 이 중attribute의 value에 접근 하고 싶을 때는 .get_attribute()메소드를 사용할 수 있다.1driver.find_element_by_css_selector(\"name of css selector\").get_attribute(\"attribute_name\") 7. element 의 위치와, 사이즈 구하기 스크롤을 내리거나, 엘리먼트의 위치와 크기를 알고 싶을 때 사용할 수 있다.12345element = driver.find_element_by_css_selector(\"name of css selector\")element.location#element 의 좌측 상단의 위치가 pixel 단위로 x, y 값의 dictionary로 보여준다.element.size#element 의 size 를 height, width를 pixel 단위로 dictionary로 보여준다.","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"},{"name":"Crawling","slug":"Python/Crawling","permalink":"https://emjayahn.github.io/categories/Python/Crawling/"}],"tags":[]},{"title":"Thread","slug":"Thread","date":"2018-10-24T03:02:50.000Z","updated":"2018-10-26T10:30:17.690Z","comments":true,"path":"2018/10/24/Thread/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/Thread/","excerpt":"","text":"Thread 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. 파이썬 기본 : Single Thread (Main Thread) threading 모듈 : main thread에서 subthread 를 생성하여 진행하는 방식 multiprocessing 모듈 : double cpu ThreadPoolExecutor : API - 멀티스레드와 멀티프로세스 동일한 형태로 디자인(Pool 클래스만 변경하면됨) threading.Thread() arguement12345Thread(group=, target=, args= , kwargs=, *, daemon=None)#target= : 실제 스레드에서 돌아가게 될 함수#args= : tuple 로 target 함수에 들어가게될 argument#kwargs= : dictionary로 target 함수에 들어가게될 argument#daemon : 데몬 스레드로 돌아갈지 여부 12345#Thread 의 메소드start(): #스레드의 실행, self 의 run() 메소드를 호출run(): #스레드가 실제로 수행하게될 작업name : #스레드의 이름threading.locals() : #해당 스레드 내부에서 사용할 로컬 변수 지정","categories":[{"name":"Python","slug":"Python","permalink":"https://emjayahn.github.io/categories/Python/"},{"name":"Thread","slug":"Python/Thread","permalink":"https://emjayahn.github.io/categories/Python/Thread/"}],"tags":[{"name":"Thread","slug":"Thread","permalink":"https://emjayahn.github.io/tags/Thread/"}]},{"title":"181024 TodayWhatILearned","slug":"181024-TodayWhatILearned","date":"2018-10-24T01:41:11.714Z","updated":"2018-10-26T04:13:11.870Z","comments":true,"path":"2018/10/24/181024-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/24/181024-TodayWhatILearned/","excerpt":"","text":"181024 TWIL오늘 할 일은 무엇인가 멀티스레딩으로 네이버 크롤링 코드 작성 오후 4시 스터디 확률 분포 공부 오늘 한 일은 무엇인가 멀티스레딩 개념을 활용한 크롤링 코드 작성 오후 4시 스터디 확률분포 공부 내일 할 일은 무엇인가 확률분포 공부 프로젝트 모임 Pandas 정리해보기 무엇을 느꼈는가 Documentation 과 각종 개념 자료들을 혼자 보고 공부하면서, 파이썬이라는 언어가 단순히 책 한권 끝냈다고 해서 끝나는 언어가 아님을 깨달았다. 기본적인 문법을 금방 익숙해져서, ‘역시 쉬운 언어인가’라고 생각했다가 오늘 다양한 자료를 찾아보고 읽어보면서 언어 하나만해도 아직 공부할게 무궁무진 하다는 것을 깨달았다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"181023 TodayWhatILearned","slug":"181023-TodayWhatILearned","date":"2018-10-23T04:31:39.118Z","updated":"2018-10-26T04:13:04.507Z","comments":true,"path":"2018/10/23/181023-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/23/181023-TodayWhatILearned/","excerpt":"","text":"181023 TWIL오늘 할 일은 무엇인가 Queue 정리 하기 확률 분포 공부 &lt;스터디&gt;멀티 스레딩 개념을 포함한 네이버 크롤링 코드짜기 오늘 한 일은 무엇인가 &lt;스터디&gt; 멀티 스레딩에 관해 이야기, 공부해볼 정보들 공유, 어떻게 코드를 구성할 것인지 의견을 나누었다. Queue 개념 정리 및 코드 구현 연습 베르누이 분포, 이항분포 정리 및 Searching 블로그 수정 내일 할 일은 무엇인가 오후 4시 멀티 스레드 관련한 내용으로 온라인 스터디 모임 예정 멀티스레딩 개념을 활용한 네이버 크롤링 코드 짜기 각종 확률 분포 및 검정 부분까지 복습 무엇을 느꼈는가 오늘 공부한 testing 시작부분부터는 그간 배웠던 것을 한꺼번에 적용한다. 배웠던 여러가지 분포를 활용하여,가설을 세우고 이 가설이 선택한 분포의 관점에서 봤을 떄, 가설을 선택할지 기각할지에 관한 내용은 점차data를 통해 prediction 을 해가는 과정에 있는 듯한 느낌을 받았다. 아직 검정 과정과 앞으로 공부하게 될 여러가지 분석 모델을 이해하기에 앞서배운 확률분포 부분의 내용이부족한듯하여, 계속 꾸준히 앞부분을 공부해야될 것 같다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[]},{"title":"Queue","slug":"Queue","date":"2018-10-22T13:49:03.552Z","updated":"2018-10-25T12:35:12.781Z","comments":true,"path":"2018/10/22/Queue/","link":"","permalink":"https://emjayahn.github.io/2018/10/22/Queue/","excerpt":"","text":"Queue 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Queue의 특징 내가 버스정류장에 서있다고 생각해보면, 버스 전용차선으로 버스가 줄지어 들어온다. 아무리 뒷차가 손님을 다 태웠다고해서, 앞에 버스가 아직 손님을 태우고 있으면 뒷 버스는 출발 하지 못한다. 이것이 바로 QUEUE FIFO : First In First Out == 선입선출 or 후입후출 front와 rear 라는 index가 각각 구조의 맨 앞과 뒤를 가리키고 있다. Data는 rear로 들어가고, front 에서 나온다. ADT empty() 라는 메소드로 Queue 에 data가 있는지 없는지 확인하게 한다. empty 이면 True, not empty 이면, False 를 return 한다.Queue.empty() returns Boolean enaueue(data) 메소드로 Queue의 rear 가 가리키고 있는 data 뒤에 data를 넣는다.Queue.enqueue (data) returns None dequeue() 메소드로 Queue의 front가 가리키고 있는 data를 반환하면서 삭제 된다.Queue.dequeue() returns data peek() 메소드로 Queue의 front가 가리키고 있는 data를 반환한다. peek은 어디까지나 확인하는 메소드 이므로, data가 삭제되지 않는다.Queue.peek() returns data 구현 1 : by python list Queue 구조를 Python에 내장 되어 있는 list를 container 로 구현한다. 123456789101112131415161718192021class Queue: def __init__(self): self.container=list() def empty(self): # ADT 를 따라서, 비어 있으면 return True 비어있지 않으면 return False if not self.container: return True return False def enqueue(self, data): # list 를 활용했으므로, list.append(data)를 활용할 수 있다. self.container.append(data) def dequeue(self): # pop 역시 list.pop()을 활용할 수 있다. return self.container.pop(0) def peek(self): # peek은 단지 front 에 어떤 데이터가 있는지 확인 하는 것 뿐이므로, 현재 리스트의 가장 앞 부분에 있는 data 를 확인하면 된다. return self.container[0] 구현 2 :","categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"https://emjayahn.github.io/categories/DataStructure/"}],"tags":[{"name":"datastructure","slug":"datastructure","permalink":"https://emjayahn.github.io/tags/datastructure/"}]},{"title":"Stack","slug":"Stack","date":"2018-10-22T13:18:03.548Z","updated":"2018-10-25T12:35:19.189Z","comments":true,"path":"2018/10/22/Stack/","link":"","permalink":"https://emjayahn.github.io/2018/10/22/Stack/","excerpt":"","text":"Stack 공부한 내용을 스스로 보기 쉽게 정리한 글입니다. Stack 의 특징 접시를 쌓듯이 데이터를 쌓아 올리는 모양의 데이터 구조 LIFO : Last In First Out == 후입선출 or 선입후출 top index 가 항상 Stack의 가장 윗부분을 가리키고 있어, 우리는 top 위치만 볼 수 있다. 단점 : stack 의 top 이 외에 밑에 쌓여져있는 데이터의 Search 가 안된다. ADT empty() 라는 메소드로 Stack 에 data가 있는지 없는지 확인하게 한다. empty 이면 True, not empty 이면, False 를 return 한다. Stack.empty() returns Boolean Stack의 top 위치에 데이터를 쌓는다. Stack.push(data) returns None Stack의 top 위치에 있는 데이터를 삭제하면서 반환한다. Stack.pop() returns data Stack의 top 위치에 있는 데이터를 반환하지만, 삭제하지 않는다. 어떤 데이터가 있는지 just 확인.Stack.peek() returns data 구현 1 : by python list Stack 구조를 Python에 내장 되어 있는 list를 container 로 삼아 구현하게 되면, 그 구현은 매우매우 쉽다. Python의 List 자료형의 대단함을 그만큼 느낀다. 이 때 List 의 index가 큰 쪽이 top 방향이다.12345678910111213141516171819202122class Stack: def __init__(self): self.container=list() def empty(self): # ADT 를 따라서, 비어 있으면 return True 비어있지 않으면 return False if not self.container: return True return False def push(self, data): # list 를 활용했으므로, list.append(data)를 활용할 수 있다. self.container.append(data) def pop(self): # pop 역시 list.pop()을 활용할 수 있다. return self.container.pop() def peek(self): # peek은 단지 top 에 어떤 데이터가 있는지 확인 하는 것 뿐이므로, 현재 리스트의 가장 마지막에 append 되어 있는 data를 확인하면 된다. return self.container[-1] 구현 2 :","categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"https://emjayahn.github.io/categories/DataStructure/"}],"tags":[{"name":"datastructure","slug":"datastructure","permalink":"https://emjayahn.github.io/tags/datastructure/"}]},{"title":"181022 TodayWhatILearned","slug":"181022-TodayWhatILearned","date":"2018-10-22T01:17:15.000Z","updated":"2018-10-26T04:12:53.705Z","comments":true,"path":"2018/10/22/181022-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/22/181022-TodayWhatILearned/","excerpt":"","text":"181022 TWIL오늘 할 일은 무엇인가 블로그 테마 바꾸기 Selenium 과제 두번째 것 (17:47 - 19:00) 확률 분포 다시 한번 정리하기 데이터구조 (PseudoLinkedList, Stack, Queue)정리 새 맥북 환경 설정 조금씩 마저 더하기 DailyScrum Upload 오늘 한 일은 무엇인가 Selenium NBA Page Crawling 해서, DataFrame으로 정리 (17:47 - 18:55) Stack 정리 Github 설정 내일 할 일은 무엇인가 확률 분포 공부 Queue 정리 무엇을 느꼈는가 맥북을 바꾸면서, 환경 설정하는데 너무 많은 시간이 든다. 틈틈히 하고 있는데도 아직 이전 맥북의 환경에서 100% 똑같은 환경을 만들지 못하고 있다. 장비를 바꿀 때, 혹은 다른 작업 환경에서 연속성을 이어 나가기 위해 나에게 맞는 환경설정도 정리할 필요성을 느낀다. 오늘 공부한 PseudoLinkedList, Stack, Queue 의 활용성에 대해 깊이 고민 해 볼 수 있어서 뿌듯하다. 특히나 PseudoLinkedList의 경우 내가 사용하지 않았던 자료구조나 class를 목적에 맞게 customize 할 수 있는 측면에서 활용성이 높다고 생각된다. 다른 사람이 작성한 class 를 잘 읽어보고, 그 활용도를 높이기 위해 overriding 하는 방법을 틈틈히 챙겨 봐야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"181019 TodayWhatILearned","slug":"181019_TodayWhatILearned","date":"2018-10-19T12:00:07.000Z","updated":"2018-10-26T04:12:47.977Z","comments":true,"path":"2018/10/19/181019_TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/19/181019_TodayWhatILearned/","excerpt":"","text":"181019오늘 한 일 Web Crawling (Requests, BS4) 공부 Github 사용할 준비 GitPages 사용 방법 공부 했던 내용, 공부할 내용 어떻게 정리 할지 고민 프로젝트 팀 회의 내일 할 일 Pandas 내용 정리, 반복 숙달 수업 내용 복습 주말 공부할 내용 계획 코딩 습관을 기르기 위해 일일코딩과 TWIL에 대해 고민 뭘 느꼈는가 크롤링 코드를 보면서 공부하고, 또 다시 스스로 짜는 것을 연습하면서 내용의 개념과 코드 자체는 어려운 것이 없으나 BeautifulSoup 이나 Requests 패키지에 담겨있는 각 메서드 들의 종류, 기능, 메서드마다 들어가는 attribute를 참고 할 줄 알아야 된다고 느꼈다. 매우 기본적인 메서드들일 수 있기에 외워야 할 수도 있겠지만 (또한 반복한다면 숙달되겠지만) 잊어버렸을 때 각 라이브러리들의 Document들을 잘 볼 줄 아는 것이 중요하다고 느꼈다. 프로젝트 OT 를 한 만큼 본격적인 프로젝트를 진행하기에 앞서 꼼꼼히 준비해 나가야겠다. 181019오늘 한 일 Web Crawling 공부 : Selenium을 활용한 크롤링 Pandas 활용하여 여러가지 data import 하고, 정렬 바꾸는 연습 Terminal 환경설정 zsh, oh-my-zsh 설치했다. git을 적극적으로 활용할 계획이므로, git사용에 유리한 zsh 설정을 마쳤다. 팀 프로젝트에 활용할 데이터 셋 탐색 github 블로그 Deploy 에 성공했다. Daily 스크럼을 작성하는 것이 하루의 계획과 정리를 하는데 유용하기에, 꾸준히 작성하면서, github 블로그에도 올려볼 계획이다. 내일 할 일 Pandas 내용 정리, 반복 숙달 Naver Article Crawling, NBA Data Crawling 확률과 통계 정리 (연속확률분포 부분) 뭘 느꼈는가 점차 학습하는 내용과 알아야 될 내용이 많아지면서, 내가 배운 것들, 알고 있는 것들. 정확히는 어떤 것에 관해 존재는 알고 있으나 내 머릿속에서 당장 꺼내서 쓰기에는 어려운 것들이 많아지고 있다. 또한, 내 머릿속에서 꺼내기 쓰기 힘들어 구글을 통해 찾고자하면 내가 기억했던, 알고 있는 정보들과 조금은 내용이 다르고, 이것을 Searching 하는데 쓰는 시간이 아깝다는 생각이 들었다. 앞으로는 매일 학습하는 내용을 바탕으로, 직접 찾기 쉬운 형태로 정리할 필요성을 느꼈다. 개인. WIKI화.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]},{"title":"181018_DailyScrum","slug":"181018-TodayWhatILearned","date":"2018-10-18T12:12:13.000Z","updated":"2018-10-26T04:12:34.775Z","comments":true,"path":"2018/10/18/181018-TodayWhatILearned/","link":"","permalink":"https://emjayahn.github.io/2018/10/18/181018-TodayWhatILearned/","excerpt":"","text":"181018오늘 한 일 Web Crawling (Requests, BS4) 공부 Github 사용할 준비 GitPages 사용 방법 공부 했던 내용, 공부할 내용 어떻게 정리 할지 고민 프로젝트 팀 회의 내일 할 일 Pandas 내용 정리, 반복 숙달 수업 내용 복습 주말 공부할 내용 계획 코딩 습관을 기르기 위해 일일코딩과 TWIL에 대해 고민 뭘 느꼈는가 크롤링 코드를 보면서 공부하고, 또 다시 스스로 짜는 것을 연습하면서 내용의 개념과 코드 자체는 어려운 것이 없으나 BeautifulSoup 이나 Requests 패키지에 담겨있는 각 메서드 들의 종류, 기능, 메서드마다 들어가는 attribute를 참고 할 줄 알아야 된다고 느꼈다. 매우 기본적인 메서드들일 수 있기에 외워야 할 수도 있겠지만 (또한 반복한다면 숙달되겠지만) 잊어버렸을 때 각 라이브러리들의 Document들을 잘 볼 줄 아는 것이 중요하다고 느꼈다. 프로젝트 OT 를 한 만큼 본격적인 프로젝트를 진행하기에 앞서 꼼꼼히 준비해 나가야겠다.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://emjayahn.github.io/categories/Diary/"}],"tags":[{"name":"TWIL","slug":"TWIL","permalink":"https://emjayahn.github.io/tags/TWIL/"}]}]}